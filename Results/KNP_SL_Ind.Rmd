---
title: "KNP_SL_Ind"
author: "Bryant Willoughby"
date: "2023-08-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(SuperLearner)
library(GpGp)
library(ggplot2)
library(caret)
```

Read in fulldata
```{r}
fulldata <- read_csv("C:/Users/Bryant Willoughby/Documents/DRUMS/raw(KNP)/Final data sets/prefire_knp_blr_grouped_standard_finaldata.csv")
```

Read in Z matrix 
```{r} 
Z <- read_csv("C:/Users/Bryant Willoughby/Documents/DRUMS/raw(KNP)/SuperLearner/KNP_NEW_meta_data.csv")

colnames(Z)[1:11] <- c("ElasticNet", "DecisionTreeRegression", "RidgeRegression", 
                      "LassoRegression", "KNeighborsRegression",   "GradientBoostingRegression", "XGBoostRegression", "BaggingRegression", 
                      "RandomForestRegression", "ExtraTreesRegression", 
                      "MultilayeredPerceptronRegression")

```

Read in Z' matrix
```{r}
Z_prime <- read_csv("C:/Users/Bryant Willoughby/Documents/DRUMS/raw(KNP)/SuperLearner/KNP_NEW_predicted_fitted_values.csv")

colnames(Z_prime)[1:11] <- c("ElasticNet", "DecisionTreeRegression", "RidgeRegression", 
                      "LassoRegression", "KNeighborsRegression",   "GradientBoostingRegression", "XGBoostRegression", "BaggingRegression", 
                      "RandomForestRegression", "ExtraTreesRegression", 
                      "MultilayeredPerceptronRegression")

```


Super Learner (Independent covariance) Results 
```{r}
#print selected variables 
selected_variables <- names(Z)[1:11]

#fit the final model using selected variables 
final_model <- lm(dnbr ~ ., data = Z[, c("dnbr", selected_variables)])

#anova(final_model)
summary(final_model) #about 74.11% of variance explained by model 

```

Use fitted coefficients to make predictions on test data 
```{r}
#make dnbr preds 
predicted_values <- predict(final_model, newdata = Z_prime[1:11])

preds <- as.vector(predicted_values)
```

# RMSE calculation
```{r}
source("C:/Users/Bryant Willoughby/Documents/DRUMS/code/week a/RMSE_function.R")
actual <- Z_prime$dnbr
predicted <- preds

val_RMSE <- RMSE(actual, predicted)

val_RMSE <- round(val_RMSE,0)
val_RMSE
```
# Create confusion matrix

Create summary table for confusion matrix computation
```{r}
data <- Z_prime[12:14] %>%
  mutate("dnbr_class" = if_else(dnbr >= 660, 7, 
                                if_else(dnbr >= 440, 6, 
                                if_else(dnbr >= 270, 5,
                                if_else(dnbr >= 100, 4,
                                if_else(dnbr >= -100, 3, 
                                if_else(dnbr >= -250, 2, 1)))))), 
         "dnbr_preds" = preds, 
         "dnbr_preds_class" = if_else(preds >= 660, 7, 
                                if_else(preds >= 440, 6, 
                                if_else(preds >= 270, 5,
                                if_else(preds >= 100, 4,
                                if_else(preds >= -100, 3, 
                                if_else(preds >= -250, 2, 1)))))))

```


```{r}
#Creates vectors having data points
expected_val <- factor(data$dnbr_class)
predicted_val <- factor(data$dnbr_preds_class)

#Creating confusion matrix
confusion_mat <- confusionMatrix(data=predicted_val, reference = expected_val)

confusion_mat
  #note: Accuracy = number of correct predictions / total number of predictions
```

Classification accuracy (CA)
```{r}
CA <- confusion_mat$overall[[1]]
CA <- round(CA, 3)
CA <- CA*100
CA
```

Classification accuracy high (CA-high)
```{r}
CA_high <- confusion_mat$table[7,7] / sum(confusion_mat$table[,7])
CA_high <- round(CA_high, 4)
CA_high <- CA_high*100
CA_high
```







